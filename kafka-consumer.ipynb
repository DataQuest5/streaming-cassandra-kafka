{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ff3ec1-3bb5-40d6-b734-91620f5d69ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0`\n",
    "import $ivy.`org.apache.spark::spark-sql-kafka-0-10:2.4.0`\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517c9413-f7c9-4fd9-8cf9-2c3133d713f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://a1b4f1c105a2:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@72343f30\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6612c415-c090-44e8-8e67-ee227cf05a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: binary, value: binary ... 5 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "  .option(\"subscribe\", \"test_prefix.testdb.transactions\")\n",
    "  .option(\"startingOffsets\", \"\"\"{\"test_prefix.testdb.transactions\":{\"0\":23}}\"\"\")\n",
    "  .option(\"endingOffsets\", \"\"\"{\"test_prefix.testdb.transactions\":{\"0\":50}}\"\"\")\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3389229d-c223-45f3-9d1c-5388c0bfdde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = [key: string, value: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputSchema: StructType = new StructType().add(\"ts\", TimestampType).add(\"str\",StringType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d57f8afc-3c4c-423b-8a9d-83a81be12933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-a5eb6b13-2408-46d0-a84f-fb904d09274b', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd11.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.IllegalStateException: Cannot fetch offset 23 (GroupId: spark-kafka-relation-e3d387c4-ada0-4a48-883b-7ca036497153-executor, TopicPartition: test_prefix.testdb.transactions-0). \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    \n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer$.org$apache$spark$sql$kafka010$InternalKafkaConsumer$$reportDataLoss0(KafkaDataConsumer.scala:642)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.reportDataLoss(KafkaDataConsumer.scala:448)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.$anonfun$get$1(KafkaDataConsumer.scala:269)\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:209)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.get(KafkaDataConsumer.scala:234)\n\tat org.apache.spark.sql.kafka010.KafkaDataConsumer.get(KafkaDataConsumer.scala:64)\n\tat org.apache.spark.sql.kafka010.KafkaDataConsumer.get$(KafkaDataConsumer.scala:59)\n\tat org.apache.spark.sql.kafka010.KafkaDataConsumer$NonCachedKafkaDataConsumer.get(KafkaDataConsumer.scala:506)\n\tat org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:113)\n\tat org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(KafkaSourceRDD.scala:104)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:255)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:405)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {test_prefix.testdb.transactions-0=23}\n\tat org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(Fetcher.java:970)\n\tat org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:490)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1259)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1187)\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.fetchData(KafkaDataConsumer.scala:470)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.fetchRecord(KafkaDataConsumer.scala:361)\n\tat org.apache.spark.sql.kafka010.InternalKafkaConsumer.$anonfun$get$1(KafkaDataConsumer.scala:251)\n\t... 34 more\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1887\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1875\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1874\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1874\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m926\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2108\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2057\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2046\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m737\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2082\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2101\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m365\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3384\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$head$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2545\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$withAction$2(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3365\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m125\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3365\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2545\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2759\u001b[39m)\n  org.apache.spark.sql.Dataset.getRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m255\u001b[39m)\n  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m292\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m746\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m705\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m714\u001b[39m)\n  ammonite.$sess.cmd11$Helper.<init>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd11$.<init>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd11$.<clinit>(\u001b[32mcmd11.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.IllegalStateException: Cannot fetch offset 23 (GroupId: spark-kafka-relation-e3d387c4-ada0-4a48-883b-7ca036497153-executor, TopicPartition: test_prefix.testdb.transactions-0). \nSome data may have been lost because they are not available in Kafka any more; either the\n data was aged out by Kafka or the topic may have been deleted before all the data in the\n topic was processed. If you don't want your streaming query to fail on such cases, set the\n source option \"failOnDataLoss\" to \"false\".\n    \u001b[39m\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer$.org$apache$spark$sql$kafka010$InternalKafkaConsumer$$reportDataLoss0(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m642\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.reportDataLoss(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m448\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.$anonfun$get$1(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m269\u001b[39m)\n  org.apache.spark.util.UninterruptibleThread.runUninterruptibly(\u001b[32mUninterruptibleThread.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.runUninterruptiblyIfPossible(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m209\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m234\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer.get$(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer$NonCachedKafkaDataConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m506\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(\u001b[32mKafkaSourceRDD.scala\u001b[39m:\u001b[32m113\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(\u001b[32mKafkaSourceRDD.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.util.NextIterator.hasNext(\u001b[32mNextIterator.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m619\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m255\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m836\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(\u001b[32mRDD.scala\u001b[39m:\u001b[32m836\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m288\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m288\u001b[39m)\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m750\u001b[39m)\n\u001b[31morg.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {test_prefix.testdb.transactions-0=23}\u001b[39m\n  org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(\u001b[32mFetcher.java\u001b[39m:\u001b[32m970\u001b[39m)\n  org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(\u001b[32mFetcher.java\u001b[39m:\u001b[32m490\u001b[39m)\n  org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(\u001b[32mKafkaConsumer.java\u001b[39m:\u001b[32m1259\u001b[39m)\n  org.apache.kafka.clients.consumer.KafkaConsumer.poll(\u001b[32mKafkaConsumer.java\u001b[39m:\u001b[32m1187\u001b[39m)\n  org.apache.kafka.clients.consumer.KafkaConsumer.poll(\u001b[32mKafkaConsumer.java\u001b[39m:\u001b[32m1115\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.fetchData(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m470\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.fetchRecord(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m361\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.$anonfun$get$1(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m251\u001b[39m)\n  org.apache.spark.util.UninterruptibleThread.runUninterruptibly(\u001b[32mUninterruptibleThread.scala\u001b[39m:\u001b[32m77\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.runUninterruptiblyIfPossible(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m209\u001b[39m)\n  org.apache.spark.sql.kafka010.InternalKafkaConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m234\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer.get$(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaDataConsumer$NonCachedKafkaDataConsumer.get(\u001b[32mKafkaDataConsumer.scala\u001b[39m:\u001b[32m506\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(\u001b[32mKafkaSourceRDD.scala\u001b[39m:\u001b[32m113\u001b[39m)\n  org.apache.spark.sql.kafka010.KafkaSourceRDD$$anon$1.getNext(\u001b[32mKafkaSourceRDD.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.util.NextIterator.hasNext(\u001b[32mNextIterator.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  scala.collection.Iterator$$anon$10.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m460\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m619\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m255\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(\u001b[32mRDD.scala\u001b[39m:\u001b[32m836\u001b[39m)\n  org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(\u001b[32mRDD.scala\u001b[39m:\u001b[32m836\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m288\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m52\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m324\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m288\u001b[39m)\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m408\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m750\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// val consoleOutput1 = df.writeStream\n",
    "//      .outputMode(\"update\")\n",
    "//      .format(\"console\")\n",
    "//      .start()\n",
    "\n",
    "// spark.streams.awaitAnyTermination()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d0bd9-cbdc-4fc6-aa3e-e43508c1ca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
